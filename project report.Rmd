---
title: "Consulting Project Report"
author: "Xiaohan Shi, Zihao Zhang, Suheng Yao"
date: "2024-11-16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmmTMB)
library(lme4)
library(caret)
library(dplyr)
library(Metrics)
library(rstanarm)
library(ggplot2)
```


```{r, echo=FALSE}
# Read in the data
df_score <- read.csv("diversity_score.csv")
df_sex <- read.csv("diversity_score_withsex.csv")
df_final <- read.csv("final_data.csv")

```


# Introduction

Our group's project is on the positional behaviors of orangutans. Orangutans are primates that live in the rain forests of Southeast Asia and are known for their red fur and high intelligence. They are also the largest arboreal mammals living in the world. They are found mainly in Borneo and Sumatra and are listed as an endangered species due to habitat loss and threats from illegal hunting. The main objective of this consulting project is to help the client analyze the positional behaviors of orangutans over developmental stages to understand how these behaviors evolve with age. The client used a measurement called Shannor Weaver Index to calculate the diversity score based on the positional behavior and try to find its relationship with age. How Shannor Weaver Index is calculated will be covered more in the method part of the report.

The original data that the client gave us contains 237 individual orangutans that had been followed for 30 years at different periods.
Each orangutan was observed for a period of time at different times, and the timing of the behavior at each location was recorded. In total, there are 77 unique positional behaviors recorded for each orangutan. These behaviors are created by the combination of three behavior groups: activity type, body position, and tree position.

In this report, we will mainly talk about the basic EDA to analyze the data, the methods used to assess the relationship, including the models used, and finally the conclusion based on the results.

# Method
## How does Shannon Weaver Index calculate?

The formula is shown below:
$H' = -\sum_{i=1}^{S} p_i \ln(p_i)$
In this formula, S represents the number of unique categories in the data, $p_i$ is the proportion, which in the client's data refers to the proportion of recorded time of each distinct behaviors of a specific orangutan in relation to this orangutan's total minutes of awake.

## Data Cleaning on Diversity Score with Sex Dataset

We noticed that there is values "M?" in the sex column, so we changed "M?" to "M". Now we can check the unique value in the sex column:
```{r, echo=FALSE}
df_sex$Sex <- gsub("M?", "M", df_sex$Sex, fixed = TRUE)
df_sex <- df_sex[!df_sex$Sex %in% c("?", "Q"), ]
unique(df_sex$Sex)
```
After cleaning the Sex column, we drew a box plots to show the difference in distribution with different sex in different age groups:
```{r, echo=FALSE}
ggplot() +
  geom_point(data=df_sex, aes(x=as.factor(Age), y=DiversityScore, fill=Sex, color=Sex), size = 0.75, shape = 1, position = position_jitterdodge()) +
  geom_boxplot(data=df_sex, aes(x=as.factor(Age), y=DiversityScore, color=Sex), alpha = 0.6) +
  labs(title="Distribution for Different Sex with Different Age Groups",
       x = "Age Group",
       y = "Diversity Score")+
  theme_classic()
```
From the plot above, there is clear distinction between distribution of diversity score for different sex. For age group 1 to 5, the median diversity score is higher for female compared to male. For age group 6, there is only female data in the dataset, and for age group 7, the median for both female and male group is similar.

According to EDA, we also think minute awake is an important predictor in modeling the diversity score, so we add Minutes.awake variable to the dataset:
```{r, echo=FALSE}
df_final_subset <- df_final[, c("Name", "Follow.Number", "Minutes.awake")]
df_final_subset <- df_final_subset %>%
  rename(
    FollowNumber = Follow.Number
  )
df_final_subset$Name <- tolower(df_final_subset$Name)
df_sex <- merge(df_sex, df_final_subset, by = c("Name", "FollowNumber"), all.x = TRUE)

glimpse(df_sex)
```
```{r, echo=FALSE}
summary_name <- df_sex %>%
  group_by(Name) %>%
  summarise(count = n(),
            max = max(DiversityScore),
            min = min(DiversityScore),
            median = median(DiversityScore),
            std = sd(DiversityScore))
std_na <- summary_name %>%
  filter(is.na(std))
print(std_na)
```
According to the standard deviation of diversity score in different name group, the name can be treated as a group variable(random effect). Those names in the above table only have 1 observation, we may consider remove those levels.(??)

## Client Models

Following client suggestions, we would like to use linear mixed effect models and build upon client's models and see if we improve the client's models. Initial client models are listed below:
```{r}
model.1 <- glmmTMB(data=df_score, 
                   DiversityScore ~  (1|Name) + (1|FollowNumber),
                   family=gaussian(link="log"))

model.2 <- glmmTMB(data=df_score, 
                   DiversityScore ~ Age + (1|Name)+ (1|FollowNumber),
                   family=gaussian(link="log"))

model.3 <- glmmTMB(data=df_score, 
                   DiversityScore ~ Age + I(Age^2) + 
                     (1|Name) + (1|FollowNumber), 
                   family=gaussian(link="log"))
```


## Check Client's Model Assumptions

We first need to check all the linear assumptions are met:
```{r}
residuals_model <- residuals(model.1, type = "pearson")

# Plot histogram of residuals
hist(residuals_model, main = "Histogram of Residuals", xlab = "Residuals")

# Q-Q plot to check normality
qqnorm(residuals_model)
qqline(residuals_model, col = "red")

fitted_values <- predict(model.1, type = "response")

# Plot residuals vs fitted values
plot(fitted_values, residuals_model, main = "Residuals vs Fitted Values",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")
```

```{r}
residuals_model <- residuals(model.2, type = "pearson")

# Plot histogram of residuals
hist(residuals_model, main = "Histogram of Residuals", xlab = "Residuals")

# Q-Q plot to check normality
qqnorm(residuals_model)
qqline(residuals_model, col = "red")

fitted_values <- predict(model.2, type = "response")

# Plot residuals vs fitted values
plot(fitted_values, residuals_model, main = "Residuals vs Fitted Values",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")
```

```{r}
residuals_model <- residuals(model.3, type = "pearson")

# Plot histogram of residuals
hist(residuals_model, main = "Histogram of Residuals", xlab = "Residuals")

# Q-Q plot to check normality
qqnorm(residuals_model)
qqline(residuals_model, col = "red")

fitted_values <- predict(model.3, type = "response")

# Plot residuals vs fitted values
plot(fitted_values, residuals_model, main = "Residuals vs Fitted Values",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")
```
Based on the assumptions check above, all three models have normal distributed residuals, no-heavy tails in qq plot and random distribution in residual vs fitted plot, indicating that all the models satisfy linear assumptions.


## Build Upon Client's Model

Here we propose three new models building upon the client models, we treat age as a categorical variable, and we also add sex and minutes.awake into the model:
```{r}
df_sex$Age <- as.factor(df_sex$Age)
df_sex <- df_sex %>%
  filter(!is.na(Minutes.awake))

newmodel.1 <- glmmTMB(data=df_sex, DiversityScore ~ Age + (1|Name), 
                   family = gaussian("log"))
summary(newmodel.1)

newmodel.2 <- glmmTMB(data=df_sex, 
                  DiversityScore ~ Age + Sex + (1|Name), 
                   family = gaussian("log"))
summary(newmodel.2)

newmodel.3 <- glmmTMB(data = df_sex, 
                   DiversityScore ~ Age + Sex + Minutes.awake + (1 | Name),
                   family = gaussian("log"))
summary(newmodel.3)

anova(newmodel.1, newmodel.2)
anova(newmodel.2, newmodel.3)
anova(newmodel.1, newmodel.3)
```
According to the chi-square test, model.3 is the best model, and minutes awakeï¼Œ age group 2, 3, 6 and 7 are statistically significant variables. Sex is not a statistically significant variable, we can consider removing it in the later model.

```{r}
newmodel.4 <- glmmTMB(data = df_sex, 
                   DiversityScore ~ Age + Minutes.awake + (1 | Name),
                   family = gaussian("log"))
summary(newmodel.4)
anova(newmodel.4, newmodel.3)
```
According to the chi-square result above, the p-value greater than 0.05, showing that we cannot reject the null hypothesis, and adding variable sex does not signifcantly improve the model performance. We can further check the performance of model 4 using AIC and MSE below.

## Model Diagnostics for Our Model
```{r}
residuals_model <- residuals(newmodel.1, type = "pearson")

# Plot histogram of residuals
hist(residuals_model, main = "Histogram of Residuals", xlab = "Residuals")

# Q-Q plot to check normality
qqnorm(residuals_model)
qqline(residuals_model, col = "red")

fitted_values <- predict(newmodel.1, type = "response")

# Plot residuals vs fitted values
plot(fitted_values, residuals_model, main = "Residuals vs Fitted Values",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")
```
```{r}
residuals_model <- residuals(newmodel.2, type = "pearson")

# Plot histogram of residuals
hist(residuals_model, main = "Histogram of Residuals", xlab = "Residuals")

# Q-Q plot to check normality
qqnorm(residuals_model)
qqline(residuals_model, col = "red")

fitted_values <- predict(newmodel.2, type = "response")

# Plot residuals vs fitted values
plot(fitted_values, residuals_model, main = "Residuals vs Fitted Values",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")
```

```{r}
residuals_model <- residuals(newmodel.3, type = "pearson")

# Plot histogram of residuals
hist(residuals_model, main = "Histogram of Residuals", xlab = "Residuals")

# Q-Q plot to check normality
qqnorm(residuals_model)
qqline(residuals_model, col = "red")

fitted_values <- predict(newmodel.3, type = "response")

# Plot residuals vs fitted values
plot(fitted_values, residuals_model, main = "Residuals vs Fitted Values",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")
```
```{r}
residuals_model <- residuals(newmodel.4, type = "pearson")

# Plot histogram of residuals
hist(residuals_model, main = "Histogram of Residuals", xlab = "Residuals")

# Q-Q plot to check normality
qqnorm(residuals_model)
qqline(residuals_model, col = "red")

fitted_values <- predict(newmodel.4, type = "response")

# Plot residuals vs fitted values
plot(fitted_values, residuals_model, main = "Residuals vs Fitted Values",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")
```

By checking the models assumptions, all three of our models satisfy the model assumptions.

## Compare Our Model to The Client's Model
```{r, warning=FALSE}
aic_model1 <- AIC(model.1)
aic_newmodel1 <- AIC(newmodel.1)
aic_model2 <- AIC(model.2)
aic_newmodel2 <- AIC(newmodel.2)
aic_model3 <- AIC(model.3)
aic_newmodel3 <- AIC(newmodel.3)
aic_newmodel4 <- AIC(newmodel.4)

predictions <- predict(model.1, type = "response")
mse_model1 <- mean((df_sex$DiversityScore - predictions)^2)

predictions <- predict(newmodel.1, type = "response")
mse_newmodel1 <- mean((df_sex$DiversityScore - predictions)^2)

predictions <- predict(model.2, type = "response")
mse_model2 <- mean((df_sex$DiversityScore - predictions)^2)

predictions <- predict(newmodel.2, type = "response")
mse_newmodel2 <- mean((df_sex$DiversityScore - predictions)^2)

predictions <- predict(model.3, type = "response")
mse_model3 <- mean((df_sex$DiversityScore - predictions)^2)

predictions <- predict(newmodel.3, type = "response")
mse_newmodel3 <- mean((df_sex$DiversityScore - predictions)^2)

predictions <- predict(newmodel.4, type = "response")
mse_newmodel4 <- mean((df_sex$DiversityScore - predictions)^2)

# Create a comparison table
comparison_table <- data.frame(
  Model = c("Model 1", "Our Model 1", "Model 2", "Our Model 2", 
            "Model 3", "Our Model 3", "Our Model 4"),
  AIC = c(aic_model1, aic_newmodel1, aic_model2, aic_newmodel2, 
          aic_model3, aic_newmodel3, aic_newmodel4),
  MSE = c(mse_model1, mse_newmodel1, mse_model2, mse_newmodel2,
          mse_model3, mse_newmodel3, mse_newmodel4)
)

# Print the table
print(comparison_table)
```
According to the AIC and MSE comparison above, our models have lower AIC and MSE, indicating that our model performs better. Although from the ANOVA table above, adding variable sex does not further improve the model, Model 3 does have smaller MSE but higher AIC compared to model 4.(??)


## Model 3 Performance Using Cross Validation

To validate the performance of the model 3 above, we propose to use k-fold cross validation:
```{r, warning=FALSE}
# Custom function to fit models
fit_glmmTMB <- function(train_data, test_data) {
  model <- glmmTMB(
    data = train_data,
    DiversityScore ~ Age + Sex + Minutes.awake + (1 | Name),
    family = gaussian(link = "log")
  )
  
  predictions <- predict(model, newdata = test_data, 
                         allow.new.levels = TRUE, type = "response")
  mse_value <- mean((test_data$DiversityScore - predictions)^2)
  
  return(mse_value)
}

# Split data into 5 folds
set.seed(724)  # For reproducibility
folds <- createFolds(df_sex$DiversityScore, k = 5, list = TRUE)

cv_results <- sapply(folds, function(test_indices) {
  test_data <- df_sex[test_indices, ]
  train_data <- df_sex[-test_indices, ]
  
  fit_glmmTMB(train_data, test_data)
})

# Calculate average MSE across folds
mean_mse <- mean(cv_results)
cat("Average MSE across folds:", mean_mse, "\n")
```

## Refit Model 3 with stan_glmer
```{r}
stanmodel_3 <- stan_glmer(
  DiversityScore ~ Age + Sex + Minutes.awake + (1 | Name),
  data = df_sex,
  family = gaussian(link = "log"),
  refresh = 0
)
```
```{r}
predictions <- posterior_predict(stanmodel_3, type = "response")
mean_prediction <- colMeans(predictions)
mse_stanmodel3 <- mean((df_sex$DiversityScore - mean_prediction)^2)
print(mse_stanmodel3)
```


## Try XGBoost Model
```{r}
# Load necessary libraries
library(xgboost)

# Preprocess data
df_sex_clean <- df_sex %>%
  mutate(Sex = ifelse(Sex == "Male", 1, 0))

df_sex_clean <- df_sex_clean %>%
  mutate(
    Age = as.numeric(factor(Age)),  # Convert Age to numeric factor
    Name = as.numeric(factor(Name))  # Convert Name to numeric factor
  )

# Define predictor matrix (X) and target variable (y)
X <- as.matrix(df_sex_clean %>% select(-DiversityScore))
y <- df_sex_clean$DiversityScore

# Split data into training and testing sets
set.seed(724)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[trainIndex, ]
X_test <- X[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]

# Convert to xgb.DMatrix
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)

# Train XGBoost model
params <- list(
  objective = "reg:squarederror",  # For regression
  eta = 0.1,  # Learning rate
  max_depth = 6,  # Tree depth
  subsample = 0.8,  # Subsampling ratio
  colsample_bytree = 0.8  # Feature sampling ratio
)
xgb_model <- xgboost(
  data = dtrain,
  params = params,
  nrounds = 100,  # Number of boosting rounds
  verbose = 1
)

# Predict on test set
y_pred <- predict(xgb_model, dtest)

# Evaluate model
mse <- mean((y_test - y_pred)^2)
cat("Mean Squared Error (XGBoost):", mse, "\n")
```





